# ğŸ“˜ Sentiment Analysis with DistilBERT  
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-orange?logo=huggingface)]()
[![Kaggle](https://img.shields.io/badge/Kaggle-GPU%20Accelerated-blue?logo=kaggle)]()
[![Python](https://img.shields.io/badge/Python-3.10-green?logo=python)]()
[![Model](https://img.shields.io/badge/Model-DistilBERT-yellow)]()

ğŸ“˜ Sentiment Analysis with DistilBERT â€” FastAPI Web App

This project demonstrates a complete end-to-end NLP pipeline using HuggingFace Transformers, DistilBERT, and FastAPI.
It includes:

âœ” Dataset loading

âœ” Tokenization

âœ” Fine-tuning DistilBERT

âœ” Saving the model

âœ” Building a modern Bootstrap UI

âœ” Deploying an API for real-time sentiment prediction

Perfect for beginners, AI/ML engineers, and portfolio projects.

ğŸš€ Features
ğŸ” 1. Sentiment Classification

Supports 2-class (Positive/Negative)

Supports 3-class (Positive/Neutral/Negative)

Uses DistilBERT, a light & fast Transformer model

âš™ï¸ 2. FastAPI Web Server

/predict â†’ JSON sentiment prediction API

/ui â†’ Modern Bootstrap UI for user input

Colored results + emojis ğŸ™‚ ğŸ˜¡ ğŸ˜

ğŸ¨ 3. Clean UI

Built with Bootstrap 5, includes:

Centered card layout

Professional design

Mobile-friendly

ğŸ“¦ 4. Easy-to-Train

Just run:

python train.py


Model is saved to:

./results/

ğŸ“ Project Structure
ğŸ“¦ DistilBERT_Sentiment_Repo
â”‚
â”œâ”€â”€ app.py                # FastAPI server with Bootstrap UI
â”œâ”€â”€ train.py              # Training script (fine-tunes DistilBERT)
â”œâ”€â”€ results/              # Saved model + tokenizer
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ vocab.txt
â”‚   â””â”€â”€ training_args.bin
â”‚
â”œâ”€â”€ train.csv             # Training dataset (your file)
â”œâ”€â”€ test.csv              # Testing dataset (your file)
â”‚
â”œâ”€â”€ README.md             # Documentation
â””â”€â”€ requirements.txt      # Python dependencies

ğŸ“¦ Installation
1ï¸âƒ£ Clone the repo
git clone https://github.com/yourusername/DistilBERT_Sentiment_Repo.git
cd DistilBERT_Sentiment_Repo

2ï¸âƒ£ Create virtual env
python3 -m venv .venv
source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ§  Training the Model

Your dataset should look like:

train.csv / test.csv
text,label
"I love this product!",1
"This is terrible.",0

Run training:
python train.py


What happens:

Tokenizer loads

Dataset is tokenized

DistilBERT is fine-tuned

Metrics (Accuracy, F1) are computed

Model is saved into ./results/

ğŸŒ Running the Web App

Start the FastAPI server:

uvicorn app:app --reload


Now visit:

ğŸ¨ UI
http://127.0.0.1:8000/ui

ğŸ§ª API (JSON)
http://127.0.0.1:8000/docs

âœ¨ UI Preview (Description)

Input box for text

Bootstrap card layout

Color-coded results:

Green = Positive ğŸ™‚

Red = Negative ğŸ˜¡

Orange = Neutral ğŸ˜

ğŸ§ª Example Predictions
Text	Output
â€œI love this!â€	POSITIVE ğŸ™‚
â€œThis is the worst.â€	NEGATIVE ğŸ˜¡
â€œIt works.â€	NEUTRAL ğŸ˜
ğŸ“ˆ Model Performance

After training you will see:

Epoch 1/2 â€“ Accuracy: 0.89, F1: 0.88
Epoch 2/2 â€“ Accuracy: 0.92, F1: 0.91

ğŸ›  Customization

You can modify:

Learning rate

Batch size

Number of labels

Model architecture

Or replace DistilBERT with:

BERT-base

RoBERTa

DeBERTa

ALBERT

ğŸ“¤ Deployment Options

You can deploy this app on:

ğŸ”¹ HuggingFace Spaces (Free)

Supports Gradio & FastAPI

ğŸ”¹ AWS EC2

Production + scaling

ğŸ”¹ Docker
docker build -t sentiment-app .
docker run -p 8000:8000 sentiment-app

â¤ï¸ Credits

Built using:

HuggingFace Transformers

FastAPI

Bootstrap

PyTorch

â­ Contribute

Pull requests welcome!
You can:

Improve UI

Add datasets

Add multi-language support

Add ONNX optimization

ğŸ¯ This Project Is Perfect For:

ML Portfolio

Job Applications

Learning Transformers

Understanding NLP pipelines

Real-time prediction apps


---

## ğŸš€ Features
- Fineâ€‘tune DistilBERT in 3â€“5 minutes on Kaggle GPU  
- HuggingFace `datasets` + `transformers`  
- Mixed precision FP16 training  
- Clean inference pipeline  
- Optional FastAPI deployment  
- Beginnerâ€‘friendly explanations

---

## ğŸ“ Project Structure
```
repo/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ app.py
â”‚â”€â”€ src/
â”‚   â””â”€â”€ inference_example.py
â”‚â”€â”€ notebook_train/
â”‚   â””â”€â”€ main.py
```

---

## ğŸ§  Model: DistilBERT
DistilBERT is 40% smaller, 60% faster, and retains 97% of BERTâ€™s accuracy.  
Ideal for learning NLP and running on free GPUs.

---

## ğŸ›  Installation
```bash
pip install -r requirements.txt
```

---

## ğŸ‹ï¸ Training (Kaggle Recommended)
Use the Kaggle notebook for:
- GPU acceleration  
- FP16 mixed precision  
- Fast dataset loading  

---

## ğŸ” Inference
```python
from transformers import pipeline

pipe = pipeline("sentiment-analysis", model="./results")
print(pipe("This movie was great!"))
```

---

## ğŸŒ FastAPI Deployment
Run:
```bash
uvicorn app:app --reload
```

---

## ğŸ“ License
MIT License
